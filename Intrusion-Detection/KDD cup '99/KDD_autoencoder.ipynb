{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.6 (tensorflow)",
      "language": "python",
      "name": "rga"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Copy of t81_558_class_14_03_anomaly.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDa7few7Bi1T",
        "colab_type": "text"
      },
      "source": [
        "# Sample code; mine is at end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI4uox7j9icG",
        "colab_type": "text"
      },
      "source": [
        "## T81-558: Applications of Deep Neural Networks\n",
        "**Module 14: Other Neural Network Techniques**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYqSYwnn9icI",
        "colab_type": "text"
      },
      "source": [
        "## Module 14 Video Material\n",
        "\n",
        "* Part 14.1: What is AutoML [[Video]](https://www.youtube.com/watch?v=TFUysIR5AB0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_14_01_automl.ipynb)\n",
        "* Part 14.2: Using Denoising AutoEncoders in Keras [[Video]](https://www.youtube.com/watch?v=4bTSu6_fucc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_14_02_auto_encode.ipynb)\n",
        "* **Part 14.3: Training an Intrusion Detection System with KDD99** [[Video]](https://www.youtube.com/watch?v=1ySn6h2A68I&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_14_03_anomaly.ipynb)\n",
        "* Part 14.4: Anomaly Detection in Keras [[Video]](https://www.youtube.com/watch?v=VgyKQ5MTDFc&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_14_04_ids_kdd99.ipynb)\n",
        "* Part 14.5: The Deep Learning Technologies I am Excited About [[Video]]() [[Notebook]](t81_558_class_14_05_new_tech.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5_UKfFB9icJ",
        "colab_type": "text"
      },
      "source": [
        "## Part 14.3: Anomaly Detection in Keras\n",
        "\n",
        "Datasets used for anomaly detection:\n",
        "\n",
        "* [Stratosphere IPS Dataset](https://www.stratosphereips.org/category/dataset.html)\n",
        "* [The ADFA Intrusion Detection Datasets (2013) - for HIDS](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-IDS-Datasets/)\n",
        "* [ITOC CDX (2009)](https://westpoint.edu/centers-and-research/cyber-research-center/data-sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpUNcXNb9icK",
        "colab_type": "text"
      },
      "source": [
        "### Read in KDD99 Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvt3tnAi9icL",
        "colab_type": "code",
        "outputId": "173771f1-a623-4b14-d534-836c6145a4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "try:\n",
        "    path = get_file('kddcup.data_10_percent.gz', origin='http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz')\n",
        "except:\n",
        "    print('Error downloading')\n",
        "    raise\n",
        "    \n",
        "print(path) \n",
        "\n",
        "# This file is a CSV, just no CSV extension or headers\n",
        "# Download from: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
        "df = pd.read_csv(path, header=None)\n",
        "\n",
        "print(\"Read {} rows.\".format(len(df)))\n",
        "# df = df.sample(frac=0.1, replace=False) # Uncomment this line to sample only 10% of the dataset\n",
        "df.dropna(inplace=True,axis=1) # For now, just drop NA's (rows with missing values)\n",
        "\n",
        "# The CSV file has no column heads, so add them\n",
        "df.columns = [\n",
        "    'duration',\n",
        "    'protocol_type',\n",
        "    'service',\n",
        "    'flag',\n",
        "    'src_bytes',\n",
        "    'dst_bytes',\n",
        "    'land',\n",
        "    'wrong_fragment',\n",
        "    'urgent',\n",
        "    'hot',\n",
        "    'num_failed_logins',\n",
        "    'logged_in',\n",
        "    'num_compromised',\n",
        "    'root_shell',\n",
        "    'su_attempted',\n",
        "    'num_root',\n",
        "    'num_file_creations',\n",
        "    'num_shells',\n",
        "    'num_access_files',\n",
        "    'num_outbound_cmds',\n",
        "    'is_host_login',\n",
        "    'is_guest_login',\n",
        "    'count',\n",
        "    'srv_count',\n",
        "    'serror_rate',\n",
        "    'srv_serror_rate',\n",
        "    'rerror_rate',\n",
        "    'srv_rerror_rate',\n",
        "    'same_srv_rate',\n",
        "    'diff_srv_rate',\n",
        "    'srv_diff_host_rate',\n",
        "    'dst_host_count',\n",
        "    'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate',\n",
        "    'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate',\n",
        "    'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_serror_rate',\n",
        "    'dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate',\n",
        "    'dst_host_srv_rerror_rate',\n",
        "    'outcome'\n",
        "]\n",
        "\n",
        "# display 5 rows\n",
        "df[0:5]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\n",
            "2146304/2144903 [==============================] - 1s 1us/step\n",
            "/root/.keras/datasets/kddcup.data_10_percent.gz\n",
            "Read 494021 rows.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>num_outbound_cmds</th>\n",
              "      <th>is_host_login</th>\n",
              "      <th>is_guest_login</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>181</td>\n",
              "      <td>5450</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>239</td>\n",
              "      <td>486</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>235</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29</td>\n",
              "      <td>29</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>219</td>\n",
              "      <td>1337</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39</td>\n",
              "      <td>39</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>217</td>\n",
              "      <td>2032</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   duration protocol_type  ... dst_host_srv_rerror_rate  outcome\n",
              "0         0           tcp  ...                      0.0  normal.\n",
              "1         0           tcp  ...                      0.0  normal.\n",
              "2         0           tcp  ...                      0.0  normal.\n",
              "3         0           tcp  ...                      0.0  normal.\n",
              "4         0           tcp  ...                      0.0  normal.\n",
              "\n",
              "[5 rows x 42 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM92uzpu9icU",
        "colab_type": "code",
        "outputId": "bdb13735-dd2b-4d29-805d-bee685560a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "df.groupby('outcome')['outcome'].count()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "outcome\n",
              "back.                 2203\n",
              "buffer_overflow.        30\n",
              "ftp_write.               8\n",
              "guess_passwd.           53\n",
              "imap.                   12\n",
              "ipsweep.              1247\n",
              "land.                   21\n",
              "loadmodule.              9\n",
              "multihop.                7\n",
              "neptune.            107201\n",
              "nmap.                  231\n",
              "normal.              97278\n",
              "perl.                    3\n",
              "phf.                     4\n",
              "pod.                   264\n",
              "portsweep.            1040\n",
              "rootkit.                10\n",
              "satan.                1589\n",
              "smurf.              280790\n",
              "spy.                     2\n",
              "teardrop.              979\n",
              "warezclient.          1020\n",
              "warezmaster.            20\n",
              "Name: outcome, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN_KP62V9icY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode a numeric column as zscores\n",
        "def encode_numeric_zscore(df, name, mean=None, sd=None):\n",
        "    if mean is None:\n",
        "        mean = df[name].mean()\n",
        "\n",
        "    if sd is None:\n",
        "        sd = df[name].std()\n",
        "\n",
        "    df[name] = (df[name] - mean) / sd\n",
        "    \n",
        "# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)\n",
        "def encode_text_dummy(df, name):\n",
        "    dummies = pd.get_dummies(df[name])\n",
        "    for x in dummies.columns:\n",
        "        dummy_name = f\"{name}-{x}\"\n",
        "        df[dummy_name] = dummies[x]\n",
        "    df.drop(name, axis=1, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dDwsM7O9icc",
        "colab_type": "code",
        "outputId": "0a599cb4-d05d-4169-c6ec-8737719778c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "# Now encode the feature vector\n",
        "\n",
        "encode_numeric_zscore(df, 'duration')\n",
        "encode_text_dummy(df, 'protocol_type')\n",
        "encode_text_dummy(df, 'service')\n",
        "encode_text_dummy(df, 'flag')\n",
        "encode_numeric_zscore(df, 'src_bytes')\n",
        "encode_numeric_zscore(df, 'dst_bytes')\n",
        "encode_text_dummy(df, 'land')\n",
        "encode_numeric_zscore(df, 'wrong_fragment')\n",
        "encode_numeric_zscore(df, 'urgent')\n",
        "encode_numeric_zscore(df, 'hot')\n",
        "encode_numeric_zscore(df, 'num_failed_logins')\n",
        "encode_text_dummy(df, 'logged_in')\n",
        "encode_numeric_zscore(df, 'num_compromised')\n",
        "encode_numeric_zscore(df, 'root_shell')\n",
        "encode_numeric_zscore(df, 'su_attempted')\n",
        "encode_numeric_zscore(df, 'num_root')\n",
        "encode_numeric_zscore(df, 'num_file_creations')\n",
        "encode_numeric_zscore(df, 'num_shells')\n",
        "encode_numeric_zscore(df, 'num_access_files')\n",
        "encode_numeric_zscore(df, 'num_outbound_cmds')\n",
        "encode_text_dummy(df, 'is_host_login')\n",
        "encode_text_dummy(df, 'is_guest_login')\n",
        "encode_numeric_zscore(df, 'count')\n",
        "encode_numeric_zscore(df, 'srv_count')\n",
        "encode_numeric_zscore(df, 'serror_rate')\n",
        "encode_numeric_zscore(df, 'srv_serror_rate')\n",
        "encode_numeric_zscore(df, 'rerror_rate')\n",
        "encode_numeric_zscore(df, 'srv_rerror_rate')\n",
        "encode_numeric_zscore(df, 'same_srv_rate')\n",
        "encode_numeric_zscore(df, 'diff_srv_rate')\n",
        "encode_numeric_zscore(df, 'srv_diff_host_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_count')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_count')\n",
        "encode_numeric_zscore(df, 'dst_host_same_srv_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_diff_srv_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_same_src_port_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_diff_host_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_serror_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_serror_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_rerror_rate')\n",
        "encode_numeric_zscore(df, 'dst_host_srv_rerror_rate')\n",
        "\n",
        "# display 5 rows\n",
        "\n",
        "df.dropna(inplace=True,axis=1)\n",
        "df[0:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>num_failed_logins</th>\n",
              "      <th>num_compromised</th>\n",
              "      <th>root_shell</th>\n",
              "      <th>su_attempted</th>\n",
              "      <th>num_root</th>\n",
              "      <th>num_file_creations</th>\n",
              "      <th>num_shells</th>\n",
              "      <th>num_access_files</th>\n",
              "      <th>count</th>\n",
              "      <th>srv_count</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>rerror_rate</th>\n",
              "      <th>srv_rerror_rate</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>diff_srv_rate</th>\n",
              "      <th>srv_diff_host_rate</th>\n",
              "      <th>dst_host_count</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>outcome</th>\n",
              "      <th>protocol_type-icmp</th>\n",
              "      <th>protocol_type-tcp</th>\n",
              "      <th>protocol_type-udp</th>\n",
              "      <th>service-IRC</th>\n",
              "      <th>service-X11</th>\n",
              "      <th>service-Z39_50</th>\n",
              "      <th>...</th>\n",
              "      <th>service-printer</th>\n",
              "      <th>service-private</th>\n",
              "      <th>service-red_i</th>\n",
              "      <th>service-remote_job</th>\n",
              "      <th>service-rje</th>\n",
              "      <th>service-shell</th>\n",
              "      <th>service-smtp</th>\n",
              "      <th>service-sql_net</th>\n",
              "      <th>service-ssh</th>\n",
              "      <th>service-sunrpc</th>\n",
              "      <th>service-supdup</th>\n",
              "      <th>service-systat</th>\n",
              "      <th>service-telnet</th>\n",
              "      <th>service-tftp_u</th>\n",
              "      <th>service-tim_i</th>\n",
              "      <th>service-time</th>\n",
              "      <th>service-urh_i</th>\n",
              "      <th>service-urp_i</th>\n",
              "      <th>service-uucp</th>\n",
              "      <th>service-uucp_path</th>\n",
              "      <th>service-vmnet</th>\n",
              "      <th>service-whois</th>\n",
              "      <th>flag-OTH</th>\n",
              "      <th>flag-REJ</th>\n",
              "      <th>flag-RSTO</th>\n",
              "      <th>flag-RSTOS0</th>\n",
              "      <th>flag-RSTR</th>\n",
              "      <th>flag-S0</th>\n",
              "      <th>flag-S1</th>\n",
              "      <th>flag-S2</th>\n",
              "      <th>flag-S3</th>\n",
              "      <th>flag-SF</th>\n",
              "      <th>flag-SH</th>\n",
              "      <th>land-0</th>\n",
              "      <th>land-1</th>\n",
              "      <th>logged_in-0</th>\n",
              "      <th>logged_in-1</th>\n",
              "      <th>is_host_login-0</th>\n",
              "      <th>is_guest_login-0</th>\n",
              "      <th>is_guest_login-1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.067792</td>\n",
              "      <td>-0.002879</td>\n",
              "      <td>0.138664</td>\n",
              "      <td>-0.04772</td>\n",
              "      <td>-0.002571</td>\n",
              "      <td>-0.044136</td>\n",
              "      <td>-0.009782</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.010552</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>-0.00564</td>\n",
              "      <td>-0.011232</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.027632</td>\n",
              "      <td>-1.521415</td>\n",
              "      <td>-1.156639</td>\n",
              "      <td>-0.464089</td>\n",
              "      <td>-0.46352</td>\n",
              "      <td>-0.24796</td>\n",
              "      <td>-0.248631</td>\n",
              "      <td>0.536987</td>\n",
              "      <td>-0.255243</td>\n",
              "      <td>-0.203633</td>\n",
              "      <td>-3.451532</td>\n",
              "      <td>-1.694313</td>\n",
              "      <td>0.599396</td>\n",
              "      <td>-0.282866</td>\n",
              "      <td>-1.022076</td>\n",
              "      <td>-0.158629</td>\n",
              "      <td>-0.464417</td>\n",
              "      <td>-0.463202</td>\n",
              "      <td>-0.252039</td>\n",
              "      <td>-0.249464</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.067792</td>\n",
              "      <td>-0.002820</td>\n",
              "      <td>-0.011578</td>\n",
              "      <td>-0.04772</td>\n",
              "      <td>-0.002571</td>\n",
              "      <td>-0.044136</td>\n",
              "      <td>-0.009782</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.010552</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>-0.00564</td>\n",
              "      <td>-0.011232</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.027632</td>\n",
              "      <td>-1.521415</td>\n",
              "      <td>-1.156639</td>\n",
              "      <td>-0.464089</td>\n",
              "      <td>-0.46352</td>\n",
              "      <td>-0.24796</td>\n",
              "      <td>-0.248631</td>\n",
              "      <td>0.536987</td>\n",
              "      <td>-0.255243</td>\n",
              "      <td>-0.203633</td>\n",
              "      <td>-3.297081</td>\n",
              "      <td>-1.600009</td>\n",
              "      <td>0.599396</td>\n",
              "      <td>-0.282866</td>\n",
              "      <td>-1.146736</td>\n",
              "      <td>-0.158629</td>\n",
              "      <td>-0.464417</td>\n",
              "      <td>-0.463202</td>\n",
              "      <td>-0.252039</td>\n",
              "      <td>-0.249464</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.067792</td>\n",
              "      <td>-0.002824</td>\n",
              "      <td>0.014179</td>\n",
              "      <td>-0.04772</td>\n",
              "      <td>-0.002571</td>\n",
              "      <td>-0.044136</td>\n",
              "      <td>-0.009782</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.010552</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>-0.00564</td>\n",
              "      <td>-0.011232</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.027632</td>\n",
              "      <td>-1.521415</td>\n",
              "      <td>-1.156639</td>\n",
              "      <td>-0.464089</td>\n",
              "      <td>-0.46352</td>\n",
              "      <td>-0.24796</td>\n",
              "      <td>-0.248631</td>\n",
              "      <td>0.536987</td>\n",
              "      <td>-0.255243</td>\n",
              "      <td>-0.203633</td>\n",
              "      <td>-3.142630</td>\n",
              "      <td>-1.505706</td>\n",
              "      <td>0.599396</td>\n",
              "      <td>-0.282866</td>\n",
              "      <td>-1.188290</td>\n",
              "      <td>-0.158629</td>\n",
              "      <td>-0.464417</td>\n",
              "      <td>-0.463202</td>\n",
              "      <td>-0.252039</td>\n",
              "      <td>-0.249464</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.067792</td>\n",
              "      <td>-0.002840</td>\n",
              "      <td>0.014179</td>\n",
              "      <td>-0.04772</td>\n",
              "      <td>-0.002571</td>\n",
              "      <td>-0.044136</td>\n",
              "      <td>-0.009782</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.010552</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>-0.00564</td>\n",
              "      <td>-0.011232</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.027632</td>\n",
              "      <td>-1.530798</td>\n",
              "      <td>-1.164758</td>\n",
              "      <td>-0.464089</td>\n",
              "      <td>-0.46352</td>\n",
              "      <td>-0.24796</td>\n",
              "      <td>-0.248631</td>\n",
              "      <td>0.536987</td>\n",
              "      <td>-0.255243</td>\n",
              "      <td>-0.203633</td>\n",
              "      <td>-2.988179</td>\n",
              "      <td>-1.411402</td>\n",
              "      <td>0.599396</td>\n",
              "      <td>-0.282866</td>\n",
              "      <td>-1.188290</td>\n",
              "      <td>-0.158629</td>\n",
              "      <td>-0.464417</td>\n",
              "      <td>-0.463202</td>\n",
              "      <td>-0.252039</td>\n",
              "      <td>-0.249464</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.067792</td>\n",
              "      <td>-0.002842</td>\n",
              "      <td>0.035214</td>\n",
              "      <td>-0.04772</td>\n",
              "      <td>-0.002571</td>\n",
              "      <td>-0.044136</td>\n",
              "      <td>-0.009782</td>\n",
              "      <td>-0.005679</td>\n",
              "      <td>-0.010552</td>\n",
              "      <td>-0.004676</td>\n",
              "      <td>-0.00564</td>\n",
              "      <td>-0.011232</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>-0.027632</td>\n",
              "      <td>-1.530798</td>\n",
              "      <td>-1.164758</td>\n",
              "      <td>-0.464089</td>\n",
              "      <td>-0.46352</td>\n",
              "      <td>-0.24796</td>\n",
              "      <td>-0.248631</td>\n",
              "      <td>0.536987</td>\n",
              "      <td>-0.255243</td>\n",
              "      <td>-0.203633</td>\n",
              "      <td>-2.833728</td>\n",
              "      <td>-1.317098</td>\n",
              "      <td>0.599396</td>\n",
              "      <td>-0.282866</td>\n",
              "      <td>-1.209066</td>\n",
              "      <td>-0.158629</td>\n",
              "      <td>-0.464417</td>\n",
              "      <td>-0.463202</td>\n",
              "      <td>-0.252039</td>\n",
              "      <td>-0.249464</td>\n",
              "      <td>normal.</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 121 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   duration  src_bytes  ...  is_guest_login-0  is_guest_login-1\n",
              "0 -0.067792  -0.002879  ...                 1                 0\n",
              "1 -0.067792  -0.002820  ...                 1                 0\n",
              "2 -0.067792  -0.002824  ...                 1                 0\n",
              "3 -0.067792  -0.002840  ...                 1                 0\n",
              "4 -0.067792  -0.002842  ...                 1                 0\n",
              "\n",
              "[5 rows x 121 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFUookBt9icf",
        "colab_type": "code",
        "outputId": "1806b884-c618-4c52-dff5-3b08eeece877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "normal_mask = df['outcome']=='normal.'\n",
        "attack_mask = df['outcome']!='normal.'\n",
        "\n",
        "df.drop('outcome',axis=1,inplace=True)\n",
        "\n",
        "df_normal = df[normal_mask]\n",
        "df_attack = df[attack_mask]\n",
        "\n",
        "print(f\"Normal count: {len(df_normal)}\")\n",
        "print(f\"Attack count: {len(df_attack)}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal count: 97278\n",
            "Attack count: 396743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeZaxLv39icj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the numeric feature vector, as it goes to the neural net\n",
        "x_normal = df_normal.values\n",
        "x_attack = df_attack.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xz3gHI29icl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_normal_train, x_normal_test = train_test_split(\n",
        "    x_normal, test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AEoSbHe9icn",
        "colab_type": "code",
        "outputId": "809dee44-a8f6-42fa-d40b-819e6adaae9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f\"Normal train count: {len(x_normal_train)}\")\n",
        "print(f\"Normal test count: {len(x_normal_test)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normal train count: 72958\n",
            "Normal test count: 24320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66pLVBEG9icq",
        "colab_type": "text"
      },
      "source": [
        "## Auto Encoders\n",
        "\n",
        "An auto encoder is a neural network that has the same number of input neurons as it does outputs.  The hidden layers of the neural network will have fewer neurons than the input/output neurons.  Because there are fewer neurons, the auto-encoder must learn to encode the input to the fewer hidden neurons.  The predictors (x) and output (y) are exactly the same in an auto encoder.  Because of this, auto encoders are said to be unsupervised.  \n",
        "\n",
        "![Simple Auto Encoder](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_13_auto_encode.png \"Simple Auto Encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ1lxnzA9icr",
        "colab_type": "code",
        "outputId": "25a3a2a1-a591-4a26-ac8d-b152f4e8f189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x_normal.shape[1], activation='relu'))\n",
        "model.add(Dense(3, activation='relu'))\n",
        "model.add(Dense(25, activation='relu'))\n",
        "model.add(Dense(x_normal.shape[1])) # Multiple output neurons\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_normal_train,x_normal_train,verbose=1,epochs=100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 72958 samples\n",
            "Epoch 1/100\n",
            "72958/72958 [==============================] - 14s 192us/sample - loss: 0.3416\n",
            "Epoch 2/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.2510\n",
            "Epoch 3/100\n",
            "72958/72958 [==============================] - 5s 64us/sample - loss: 0.2232\n",
            "Epoch 4/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.2204\n",
            "Epoch 5/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.2031\n",
            "Epoch 6/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1966\n",
            "Epoch 7/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1979\n",
            "Epoch 8/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.2103\n",
            "Epoch 9/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.2016\n",
            "Epoch 10/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1949\n",
            "Epoch 11/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1985\n",
            "Epoch 12/100\n",
            "72958/72958 [==============================] - 4s 62us/sample - loss: 0.1929\n",
            "Epoch 13/100\n",
            "72958/72958 [==============================] - 5s 65us/sample - loss: 0.1916\n",
            "Epoch 14/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1883\n",
            "Epoch 15/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1912\n",
            "Epoch 16/100\n",
            "72958/72958 [==============================] - 5s 64us/sample - loss: 0.1821\n",
            "Epoch 17/100\n",
            "72958/72958 [==============================] - 5s 64us/sample - loss: 0.1792\n",
            "Epoch 18/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1780\n",
            "Epoch 19/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1705\n",
            "Epoch 20/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1629\n",
            "Epoch 21/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1791\n",
            "Epoch 22/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1531\n",
            "Epoch 23/100\n",
            "72958/72958 [==============================] - 4s 62us/sample - loss: 0.1566\n",
            "Epoch 24/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1568\n",
            "Epoch 25/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1436\n",
            "Epoch 26/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1469\n",
            "Epoch 27/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1522\n",
            "Epoch 28/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1437\n",
            "Epoch 29/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1438\n",
            "Epoch 30/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1403\n",
            "Epoch 31/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1381\n",
            "Epoch 32/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1439\n",
            "Epoch 33/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1377\n",
            "Epoch 34/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1378\n",
            "Epoch 35/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1403\n",
            "Epoch 36/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1394\n",
            "Epoch 37/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1359\n",
            "Epoch 38/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1367\n",
            "Epoch 39/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1348\n",
            "Epoch 40/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1314\n",
            "Epoch 41/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1354\n",
            "Epoch 42/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1316\n",
            "Epoch 43/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1294\n",
            "Epoch 44/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1358\n",
            "Epoch 45/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1291\n",
            "Epoch 46/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1292\n",
            "Epoch 47/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1279\n",
            "Epoch 48/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1257\n",
            "Epoch 49/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1282\n",
            "Epoch 50/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1286\n",
            "Epoch 51/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1271\n",
            "Epoch 52/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1248\n",
            "Epoch 53/100\n",
            "72958/72958 [==============================] - 5s 64us/sample - loss: 0.1241\n",
            "Epoch 54/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1236\n",
            "Epoch 55/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1216\n",
            "Epoch 56/100\n",
            "72958/72958 [==============================] - 5s 64us/sample - loss: 0.1195\n",
            "Epoch 57/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1243\n",
            "Epoch 58/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1217\n",
            "Epoch 59/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1212\n",
            "Epoch 60/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1170\n",
            "Epoch 61/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1157\n",
            "Epoch 62/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1147\n",
            "Epoch 63/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1163\n",
            "Epoch 64/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1171\n",
            "Epoch 65/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1124\n",
            "Epoch 66/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1121\n",
            "Epoch 67/100\n",
            "72958/72958 [==============================] - 4s 62us/sample - loss: 0.1104\n",
            "Epoch 68/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1103\n",
            "Epoch 69/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1134\n",
            "Epoch 70/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1106\n",
            "Epoch 71/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1117\n",
            "Epoch 72/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1052\n",
            "Epoch 73/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1102\n",
            "Epoch 74/100\n",
            "72958/72958 [==============================] - 4s 62us/sample - loss: 0.1075\n",
            "Epoch 75/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1087\n",
            "Epoch 76/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1074\n",
            "Epoch 77/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1068\n",
            "Epoch 78/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1023\n",
            "Epoch 79/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1079\n",
            "Epoch 80/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1042\n",
            "Epoch 81/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1023\n",
            "Epoch 82/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1017\n",
            "Epoch 83/100\n",
            "72958/72958 [==============================] - 5s 62us/sample - loss: 0.1044\n",
            "Epoch 84/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.1006\n",
            "Epoch 85/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.0995\n",
            "Epoch 86/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.1014\n",
            "Epoch 87/100\n",
            "72958/72958 [==============================] - 5s 65us/sample - loss: 0.0988\n",
            "Epoch 88/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.0991\n",
            "Epoch 89/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1014\n",
            "Epoch 90/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.0975\n",
            "Epoch 91/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1039\n",
            "Epoch 92/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.1001\n",
            "Epoch 93/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.0945\n",
            "Epoch 94/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.0963\n",
            "Epoch 95/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.0987\n",
            "Epoch 96/100\n",
            "72958/72958 [==============================] - 5s 63us/sample - loss: 0.0966\n",
            "Epoch 97/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.0922\n",
            "Epoch 98/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.0970\n",
            "Epoch 99/100\n",
            "72958/72958 [==============================] - 4s 61us/sample - loss: 0.0971\n",
            "Epoch 100/100\n",
            "72958/72958 [==============================] - 4s 60us/sample - loss: 0.0934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f33676705f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgD-5HlL9ict",
        "colab_type": "code",
        "outputId": "5aefb1e1-f7e3-4bbb-b6e1-7fd824403923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pred = model.predict(x_normal_test)\n",
        "score1 = np.sqrt(metrics.mean_squared_error(pred,x_normal_test))\n",
        "pred = model.predict(x_normal)\n",
        "score2 = np.sqrt(metrics.mean_squared_error(pred,x_normal))\n",
        "pred = model.predict(x_attack)\n",
        "score3 = np.sqrt(metrics.mean_squared_error(pred,x_attack))\n",
        "print(f\"Out of Sample Normal Score (RMSE): {score1}\".format(score1))\n",
        "print(f\"All Sample Normal Score (RMSE): {score2}\")\n",
        "print(f\"Attack Underway Score (RMSE): {score3}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Out of Sample Normal Score (RMSE): 0.2973644684146504\n",
            "All Sample Normal Score (RMSE): 0.2971085517610443\n",
            "Attack Underway Score (RMSE): 0.6064449515112275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GVtGobVBeO2",
        "colab_type": "text"
      },
      "source": [
        "# Alik604's code "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFoIj3C09icv",
        "colab_type": "text"
      },
      "source": [
        "## Let's make it \"deeper\" and add some batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syqUevpPAC3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c3e35dc-56bc-4fd3-fd53-2997464bdd69"
      },
      "source": [
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(60, input_dim=x_normal.shape[1], activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(3, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(60, activation='relu'))\n",
        "model.add(Dense(x_normal.shape[1])) # Multiple output neurons\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_normal_train,x_normal_train,verbose=1,epochs=100)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 72958 samples\n",
            "Epoch 1/100\n",
            "72958/72958 [==============================] - 9s 125us/sample - loss: 0.3534\n",
            "Epoch 2/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.2887\n",
            "Epoch 3/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.2513\n",
            "Epoch 4/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.2288\n",
            "Epoch 5/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.2110\n",
            "Epoch 6/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.2036\n",
            "Epoch 7/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.1864\n",
            "Epoch 8/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.1688\n",
            "Epoch 9/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1595\n",
            "Epoch 10/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1477\n",
            "Epoch 11/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1332\n",
            "Epoch 12/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1369\n",
            "Epoch 13/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1407\n",
            "Epoch 14/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.1337\n",
            "Epoch 15/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1274\n",
            "Epoch 16/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1363\n",
            "Epoch 17/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.1329\n",
            "Epoch 18/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1288\n",
            "Epoch 19/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1659\n",
            "Epoch 20/100\n",
            "72958/72958 [==============================] - 9s 126us/sample - loss: 0.1568\n",
            "Epoch 21/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.1198\n",
            "Epoch 22/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1376\n",
            "Epoch 23/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.1187\n",
            "Epoch 24/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.1107\n",
            "Epoch 25/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1468\n",
            "Epoch 26/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1524\n",
            "Epoch 27/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.1326\n",
            "Epoch 28/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.1564\n",
            "Epoch 29/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.1348\n",
            "Epoch 30/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1240\n",
            "Epoch 31/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1162\n",
            "Epoch 32/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1125\n",
            "Epoch 33/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1137\n",
            "Epoch 34/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1148\n",
            "Epoch 35/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.1088\n",
            "Epoch 36/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.1115\n",
            "Epoch 37/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.1026\n",
            "Epoch 38/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1045\n",
            "Epoch 39/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.0997\n",
            "Epoch 40/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.0985\n",
            "Epoch 41/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1008\n",
            "Epoch 42/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.0961\n",
            "Epoch 43/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.0998\n",
            "Epoch 44/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.0931\n",
            "Epoch 45/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1038\n",
            "Epoch 46/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.0910\n",
            "Epoch 47/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.0891\n",
            "Epoch 48/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.0846\n",
            "Epoch 49/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.0868\n",
            "Epoch 50/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.0848\n",
            "Epoch 51/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.0895\n",
            "Epoch 52/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.0814\n",
            "Epoch 53/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.0866\n",
            "Epoch 54/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.0807\n",
            "Epoch 55/100\n",
            "72958/72958 [==============================] - 9s 125us/sample - loss: 0.0718\n",
            "Epoch 56/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.0733\n",
            "Epoch 57/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.0742\n",
            "Epoch 58/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.0747\n",
            "Epoch 59/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.0697\n",
            "Epoch 60/100\n",
            "72958/72958 [==============================] - 9s 117us/sample - loss: 0.0701\n",
            "Epoch 61/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.0719\n",
            "Epoch 62/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.0766\n",
            "Epoch 63/100\n",
            "72958/72958 [==============================] - 8s 116us/sample - loss: 0.0653\n",
            "Epoch 64/100\n",
            "27520/72958 [==========>...................] - ETA: 5s - loss: 0.0868"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-10e148dc7b0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Multiple output neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_normal_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_normal_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAVQmF8AAC6E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "90f069f6-87b1-40ed-87f1-5de1af11fb03"
      },
      "source": [
        "pred = model.predict(x_normal_test)\n",
        "score1 = np.sqrt(metrics.mean_squared_error(pred,x_normal_test))\n",
        "pred = model.predict(x_normal)\n",
        "score2 = np.sqrt(metrics.mean_squared_error(pred,x_normal))\n",
        "pred = model.predict(x_attack)\n",
        "score3 = np.sqrt(metrics.mean_squared_error(pred,x_attack))\n",
        "print(f\"Out of Sample Normal Score (RMSE): {score1}\".format(score1))\n",
        "print(f\"All Sample Normal Score (RMSE): {score2}\")\n",
        "print(f\"Attack Underway Score (RMSE): {score3}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Out of Sample Normal Score (RMSE): 3.16290053474504\n",
            "All Sample Normal Score (RMSE): 3.038798099868412\n",
            "Attack Underway Score (RMSE): 1.5017461836201043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3D0oBuwAC9g",
        "colab_type": "text"
      },
      "source": [
        "## Test less constriction (center for AE) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qW3T1VjADCR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6768511-b6e1-4452-93f5-16588febadeb"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(60, input_dim=x_normal.shape[1], activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(20, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(60, activation='relu'))\n",
        "model.add(Dense(x_normal.shape[1])) # Multiple output neurons\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_normal_train,x_normal_train,verbose=1,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 72958 samples\n",
            "Epoch 1/100\n",
            "72958/72958 [==============================] - 9s 127us/sample - loss: 0.2983\n",
            "Epoch 2/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.2370\n",
            "Epoch 3/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.2057\n",
            "Epoch 4/100\n",
            "72958/72958 [==============================] - 9s 119us/sample - loss: 0.1815\n",
            "Epoch 5/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.1606\n",
            "Epoch 6/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1459\n",
            "Epoch 7/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.1369\n",
            "Epoch 8/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.1316\n",
            "Epoch 9/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.1412\n",
            "Epoch 10/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.1277\n",
            "Epoch 11/100\n",
            "72958/72958 [==============================] - 9s 125us/sample - loss: 0.1164\n",
            "Epoch 12/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.1178\n",
            "Epoch 13/100\n",
            "72958/72958 [==============================] - 9s 126us/sample - loss: 0.1122\n",
            "Epoch 14/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.1032\n",
            "Epoch 15/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.1043\n",
            "Epoch 16/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.1131\n",
            "Epoch 17/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.0995\n",
            "Epoch 18/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.0997\n",
            "Epoch 19/100\n",
            "72958/72958 [==============================] - 9s 118us/sample - loss: 0.1022\n",
            "Epoch 20/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.1032\n",
            "Epoch 21/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.1033\n",
            "Epoch 22/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.0916\n",
            "Epoch 23/100\n",
            "72958/72958 [==============================] - 9s 122us/sample - loss: 0.1007\n",
            "Epoch 24/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.1055\n",
            "Epoch 25/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.0946\n",
            "Epoch 26/100\n",
            "72958/72958 [==============================] - 9s 128us/sample - loss: 0.0925\n",
            "Epoch 27/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.0832\n",
            "Epoch 28/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.0872\n",
            "Epoch 29/100\n",
            "72958/72958 [==============================] - 9s 124us/sample - loss: 0.0842\n",
            "Epoch 30/100\n",
            "72958/72958 [==============================] - 9s 123us/sample - loss: 0.0860\n",
            "Epoch 31/100\n",
            "72958/72958 [==============================] - 9s 120us/sample - loss: 0.0900\n",
            "Epoch 32/100\n",
            "72958/72958 [==============================] - 9s 121us/sample - loss: 0.0768\n",
            "Epoch 33/100\n",
            "35456/72958 [=============>................] - ETA: 4s - loss: 0.1370"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXRJmFrQAOUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(x_normal_test)\n",
        "score1 = np.sqrt(metrics.mean_squared_error(pred,x_normal_test))\n",
        "pred = model.predict(x_normal)\n",
        "score2 = np.sqrt(metrics.mean_squared_error(pred,x_normal))\n",
        "pred = model.predict(x_attack)\n",
        "score3 = np.sqrt(metrics.mean_squared_error(pred,x_attack))\n",
        "print(f\"Out of Sample Normal Score (RMSE): {score1}\".format(score1))\n",
        "print(f\"All Sample Normal Score (RMSE): {score2}\")\n",
        "print(f\"Attack Underway Score (RMSE): {score3}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvtQpBacB9D7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm-ljElB9Vf",
        "colab_type": "text"
      },
      "source": [
        "# Test a Variational AutoEncoder (VAE) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uilwxyQxB9Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow_probability.layers import dense_variational, dense_variational_v2\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(60, input_dim=x_normal.shape[1], activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(dense_variational(20))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(dense_variational_v2(20))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(60, activation='relu'))\n",
        "model.add(Dense(x_normal.shape[1])) # Multiple output neurons\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_normal_train,x_normal_train,verbose=1,epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFtF__gVB9b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZZPeLJwB9fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1wvCoiwB9h6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "4e874fb8-60d6-44cb-ebf5-58bc5b467182"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfp-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/25/4d9d58c16aa294853fb3b8ece1df1e94d104dc21ce16068d9eb8345603d0/tfp_nightly-0.10.0.dev20200302-py2.py3-none-any.whl (3.5MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: cloudpickle>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly) (1.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from tfp-nightly) (4.4.1)\n",
            "Collecting gast>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tfp-nightly) (1.12.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tfp-nightly\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "Successfully installed gast-0.3.3 tfp-nightly-0.10.0.dev20200302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OrrcqNODnOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "abd6c5ff-4281-4fb6-9d68-ec295c255b1d"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dm-sonnet==1.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/c7/e32a7d83724f26e921dcdd7ddd8f30e6e92cb4e68c740960307616b6ada8/dm_sonnet-1.23-py3-none-any.whl (616kB)\n",
            "\r\u001b[K     |▌                               | 10kB 32.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 40kB 5.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 6.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 81kB 6.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 92kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 122kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 133kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 163kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 184kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 204kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 215kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 225kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 245kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 256kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 266kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 276kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 286kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 296kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 307kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 317kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 327kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 337kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 358kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 368kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 378kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 389kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 399kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 409kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 419kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 430kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 440kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 450kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 460kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 471kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 481kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 491kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 501kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 512kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 522kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 532kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 542kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 552kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 563kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 573kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 583kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 593kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 604kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 614kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from dm-sonnet==1.23) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from dm-sonnet==1.23) (1.12.0)\n",
            "Installing collected packages: dm-sonnet\n",
            "  Found existing installation: dm-sonnet 1.35\n",
            "    Uninstalling dm-sonnet-1.35:\n",
            "      Successfully uninstalled dm-sonnet-1.35\n",
            "Successfully installed dm-sonnet-1.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRpZLTc0DozC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}